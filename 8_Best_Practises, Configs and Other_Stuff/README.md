<h1 align="center">Best Practises, Configs and Other stuff</h1>


## Description

Что же, это последний урок перед финальным заданием. Здесь вы познакомитесь с различными рекомендациями по настройке конфигов, какие вообще конфиги наиболее часто встречаются,
ну и конечно же с другими различными best practises.


## Cluster configuration

Что же, начнём с довольно интересной темы: как настроить свой кластер для определенной работы. Сразу скажу что в рамках датабрикс запускается один executor на одном worker, а 
это значит что вы не можете менеджить количество executors и их ресурсы через конфиги, только через выбор самого worker. Вот вырезка из статьи от самого databricks(
Databricks runs one executor per worker node; therefore the terms executor and worker are used interchangeably in the context of the Databricks architecture).
Вот статья про настройку: https://habr.com/ru/company/otus/blog/529100/?ysclid=l91eg9e2n6388772772.

Теперь немного от себя(от Microsoft Azure):

1) Ваша задача сложная аналитика: тогда вам подойдёт один, но очень мощный executor. Все достаточно просто, в аналитике вы скорее всего будете часто аггрегировать по 
разным полям, далеть оконки и т.д. Из-за этого будет постоянное перемещение данных с одного executor в другой. А если executor один то проблемы нет. Главное
чтобы GC не выкидывал вам ошибки, поэтому ресурсов на кластере должно быть больше чем объем данных.

2) Ваша задача обычный ETL: любой кластер сойдёт, от одного большого до кучи маленьких. Тут надо тестировать.

3) Большой ETL: тоже самое что и с аналитикой. Много join приведут к большому количеству shuffle, поэтому как вариант надо юзать очень большой executor. Опять же,
чтобы GC не ругался даём кластеры ресурсов больше чем данных.

4) Машинное обучение: ну тут что душе угодно, кроме кучи маленьких. В целом один большой или много но средних по размеру вполне подойдёт. Надо тестить.

Вообще скажу так, всё надо тестить. Best practises не дают гарантии, всё познаётся на тестах.

## Coalesce and repartition

Вы уже видели их и что они делают. Сейчас немного повторим и углубимся в суть. Обе этих функции используются для регулирования числа партиций в вашем коде. Coalesce 
может только уменьшать, в то время как repartition может уменьшать и увеличивать. Тем не менее есть различие в подходе их работы. Coalesce старается работать на совмещение
партиций. Это значит, что если например у вас 3 executors и на первых двух 3 партиции, а на последнем 5 партиций и вы задали coalesce(9). В этом случае на последнем
executor coalesce впихнёт те две лишние партиции в другие свободные две, тем самым две партиции на этом executor станут больше. Вроде как бы всё гуд, везде по 3 партиции,
обошлись без shuffle. Тем не менее, это может привести к перекосу данных. А вдруг до этого все партиции были равными, теперь у вас есть две огромные партиции да ещё и 
на одном executor. Не очень то и хорошо, поэтому coalesce надо использовать с понимаем этого факта. Repartiotion же делает полный shuffle. Вы можете туда передать как
количество партиций на выходе(будет рандомный shuffle), так и по какому ключу делать shuffle(тогда все данные с одним ключом в одной партиции), так и количество партиций
и по какому ключу. Вот пример: https://sparkbyexamples.com/spark/spark-repartition-vs-coalesce/.
Используются эти две штуки для решения проблем перекоса данных. Вы видите что у вас какие-то партиции сильно больше других после джойна а вам ещё делать какие-то операции
по типу фильтр, селект и udf? Найдите ключ по которому датасет равномерно распределен или посчитайте нужное количество партиций и выровняйте их. Ещё юзаются при сохранении
файлов, например надо всё сохранить как один файл, юзайте coalesce(1). Чуть более подробно про то как они юзаются при сохранении файлов будет ниже.
Note: вообще короче есть прикол один, расскажу его тут. Если вы например делаете джойн, потом например фильтр и селект, потом сохраняете все в один файл с помощью coalesce, 
то после джойн спарк сделает 1 партицию, т.к. увидит что зачем мне лишний раз перемещать данные, сразу сделаю 1 партицию. И вот тут ваше приложение умрёт, ибо
вам то ещё делать фильтр и селект перед сохранением данных, а партиция то уже одна да ещё и огромная. Чтобы такого не происходило, необходимо разбивать на большее число stages
вашу программу, путём того же repartition вместо coalesce(reapartition вызывает shuffle, а значит разбивает код на stages). Поэтому предпоследний этап(где 
селект и фильтр) будет работать в spark.sql.shuffle.partitions параллельных потоков, а в последнем произойдёт сохранение в 1 файл(ну или сколько зададите). Тут 
на самом деле вопрос что лучше, оставить с coalesce или же переписать с repartition, разные кейсы бывают, поэтому надо тестировать.

## spark.sql.shuffle.partitions

Конфиг, с которым вы будете работать чаще всего. Задаёт количество выходных партиций после широких трансформаций таких как джойн. Вообще надо следить за ним очень аккуратно,
по дефолту он 200, но для каждой отдельной задачи он должен быть свой.

## Broadcast variables and Accumulators

Если бродкаст переменные топ 1 вопрос на собесе(реально часто встречается) и на практике тоже могут попасться, то вот аккумуляторы очень редкий вопрос т.к. на практике вы
их встретите с вероятностью 0.1%.

Accumulators: https://sparkbyexamples.com/pyspark/pyspark-accumulator-with-example/.
Broadcast variables: https://sparkbyexamples.com/spark/spark-broadcast-variables/.
Если кто-то был невнимателен, то про broadcast расскажу здесь: суть в том что когда вы используете какую-то переменную например при фильтре, то эта переменная отправляется
вместе с задачей на исполнение. Как вы помните, одна задача-одна партиция, то есть если вы фильтруете датафрейм с 500 партициями, эта переменная отправится 500 раз. Звучит
ужасно, тем не менее это не критично с примитивами как Int или String, тем не менее когда дело доходит например до словарей размером в N элементов, то уже не так
круто. Поэтому можно сделать переменную бродкаст, тем самым она отправится в начале stage(вроде) в storage memory и там будет храниться. Поэтому отправится она не 500 раз,
а всего лишь m раз(где m число executors). Профит? Профит. 
Аккумуляторы же оч редкие, так что главное знать зачем они, детали уже не важны.

## PySpark serializator

Вообще, вы уже читали про Спарк сериализаторы. Так вот, это была скала, и большинство даже считают что и в PySpark те же самые. Но это не так. Если попытаться словить
ошибку на сериализаторе(поверьте, вы когда-нибудь её словите), то в ошибке будет написано про pickle, тот самый питоновский сериализатор. В питоне есть ещё один, marshal,
который как гласят гайды быстрее pickle. Кстати как раз из-за особенностей PySpark и устройства его сериализаторов, есть некоторые ограничения которых нет в скале. Например
для foreach, куда в скале вы можете передавать объекты созданные вне foreach, а вот питоновский pickle будет кричать что поток заблочен(сам столкнулся с этим). 
Вообще тут наверное стоит сказать, что для более серьезной разработки всё таки скала в разы лучше, но вот читабельность и простота всё же за PySpark. Есть куча нюансов 
из-за которых те или иные вещи в питоне не работают, углубляться не буду ибо сам не до конца понимаю, но не раз замечал как один и тот же по идее код написаный на скале и 
питоне выдаёт разные даже планы запроса(на скале очевидно лучше).

## AQE

Вы уже не раз слышали про эту чудо машину. 
Статья: https://sparkbyexamples.com/spark/spark-adaptive-query-execution/.
Разумеется штука крутая и полезная, но она не может решить все проблемы самостоятельно(иначе зачем мы нужны)))), поэтому эта вещь служит для того, чтобы если вы вдруг про-
пустили что-то или например данные один раз пришли другие, ваш процесс не упал а сработал как надо. Ну а вообще такие вещи как выравнивание данных после Shuffle делает
даже лучше человека(ибо всё же она знает все метрики на рантайме).

## Почему Dataframe и Dataset, а не RDD

Вообще всё очевидно, оптимизатор не оптимизирует RDD, да и писать на RDD API в 1000 раз сложнее. Поэтому везде где можно юзается dataframe, dataset же нету в pyspark ибо
он только для компилируемых языков, то есть для скалы. RDD используется только в одном случае: если файл txt или другой вообще не структурируемый. 

## Как считывать файлы

Сначала своими словами.
spark.default.parallelism (default: Total No. of CPU cores) — для RDD, количество партиций после shuffle, ну и как увидим для вычисления размера
spark.sql.files.maxPartitionBytes (128 mb default) — размер данных при чтении с файла
spark.sql.files.openCostInBytes (default: 4 MB) — размер дополнительных данных при чтении с файла(учитывается только после того как мы упаковали часть файла, пример: 
размер партиции 128, стоимость открытия 4. Берется файл 40мб, кладётся в партицию  и добавляется 4мб, затем берётся другой файл 40мб, он помещается значит кладём и его 
и добавляем сверху 4мб, затем берется ещё один файл 40мб, он помещается значит кладём и его, итого 3 файла)
maxSplitBytes = Minimum(maxPartitionBytes, bytesPerCore)
bytesPerCore = (Sum of sizes of all data files + No. of files * openCostInBytes) / default.parallelism

Теперь чуть более подробно. В общем, у вас есть разные файлы. После вычисления параметров, вы получите размер одной партиции. После этого, спарк посмотрит на файлы и
если они большего размера чем полученный размер одной партиции и сплитэбл, то он будет делить этот файл на куски равные размеру партиции, пока последний кусок не 
будет меньше либо равен размеру партиции. Если файлы меньше партиции или они не сплитэбл то он их так и оставит. Ну и вот, из полученных по итогу файлов он будет фармиро-
вать партиции. При этом после того как он будет добавлять кусочек файла или целый файл в партицию, то он будет прибавлять spark.sql.files.openCostInBytes. Ну и как бы тут
у меня у самого есть вопрос, а что если файл паркет, он же после того как данные считает получит данных в разы больше(паркет же сжимает). Тут я конечно бессилен ибо сам
не знаю, оставляю этот вопрос открытым. Вообще вы спросите а зачем это надо? Чтобы знать какое число партиций будет у вас после прочтения файлов и на самом деле
менеджить это число надо почти всегда хотя бы приблизительно.

Вот статья: https://dzone.com/articles/guide-to-partitions-calculation-for-processing-dat.

## Как сохранять файлы

Тут будет статья в которой всё круто расписано.
Статья: https://mungingdata.com/apache-spark/partitionby/.

## UDF

Когда не хватает встроенного функционала, на помощь приходят UDF. По сути обычная функция, только для работа с колонками. Есть три вида: PySpark UDF, Pandas UDF, Scala UDF.
В целом очевидно самая быстрая Scala UDF, на втором месте Pandas UDF т.к. она работает с векторами, ну и самая медленная PySpark UDF т.к. он работает по элементно.
На Scala очевидно вы будете юзать ток Scala UDF, но на PySpark чаще всего PySpark UDF. Тем не менее, если выполнение UDF становится проблемой(по производительности), то
стоит заменить на Pandas UDF. Ну а вообще лучше их не юзать, ибо оптимизитор их не оптимизирует(он же не знает что вы там напишите), да и те же PySpark UDF реально медлительны.

Есть еще очень эффективный вариант если нужен именно PySpark и нужна UDF. Как, надеюсь, все в курсе секрет скорости исполнения кода Python - исполнять код не Python. В общем случаее мы просто подключаем библиотеки C/C++ Rust и т.д. Ни кто не мешает сделать то же самое для библиотеки исполниемой на JVM. Просто создается jar на языке совместимом с JVM и способном работать с PySpark. В данном случае это Scala и подключается в скрипт Python. Более подробно и красиво тут [Using Scala UDFs in PySpark](https://medium.com/wbaa/using-scala-udfs-in-pyspark-b70033dd69b9).
